{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaKNwdyvsN8/uDupGidyYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi99-ut/RL/blob/main/Introduction_to_RL_Chapter_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 1"
      ],
      "metadata": {
        "id": "j7HHPEhC2nvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Reading Notes of Introduction to Reinforcement Learning by Sutton and Barto"
      ],
      "metadata": {
        "id": "oinOXRMG2_4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the most distinctive feature of RL?\n",
        "*   Trial and Error\n",
        "*   Delayed Reward\n",
        "\n",
        "To obtain Reward RL agent must prefer action it has tried in the past and has found effective, but it also need to explore if there are highre reward. So the challenge for RL agentis to trade off between Explore and Exploit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XrpVVO7Z3G1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: ***How will an agent know how much to explore and when to exploit? what is the mathematical focrmulation of this?***"
      ],
      "metadata": {
        "id": "9Qy-P4SJ45wJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL Agents have:\n",
        "*   Explicit Goals\n",
        "*   Can sense aspect of their environment\n",
        "*   Can choose action to influence the environment\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pv2SyZJM3__p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elements of RL\n",
        "Beyon Agent and Environment. Following are the subagents of RL:\n",
        "- Policy\n",
        "- Rewards Signal\n",
        "- Value function\n",
        "- Model"
      ],
      "metadata": {
        "id": "SsaTM1FR4Y-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Policy:**\n",
        "Policy can be thought of a way to map **State** to and **Action**. It can be stochastics i.e. specifying probablities of each action\n",
        "\n",
        "Policy{Perceived State:--->Action}"
      ],
      "metadata": {
        "id": "lCoG7nTt6h3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: ***How are policy determined? Are policies defined by user or is it defined by environment in practical setting.***"
      ],
      "metadata": {
        "id": "QEUOHyab7D8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward**\n",
        "Reward Signal defines the goal of an RL agent, Agents sole objective is to maximize the total rewards in the long run.\n",
        "\n",
        "Reward signal can be thought of in immediate sense (short term)\n",
        "\n",
        "For example: If a policy maps a low value action for a given situtaion then in future policy may be updated for some other action"
      ],
      "metadata": {
        "id": "c7ZRWYG271Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value Function** This can be interpreted as what is good in the long run.\n",
        "A value of a state is the total amount of reward an agent expects to accumulate over the future starting from that state.\n",
        "\n",
        "While evaluating and making decision, value is what is being looked at i.e. Seek action that results in highest value (not highest reward signal)\n",
        "\n",
        "Reward : Short term (Instant Gratification)\\\n",
        "Value: Long term (Delayed Gratification)"
      ],
      "metadata": {
        "id": "KPhphah1ACTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**\n",
        "Models can be thought of given a state and action model predicts next state and action\n",
        "\n",
        "$(State, Action)_{current}$--> $(Model)$-->$(State,Action)_{next}$\n",
        "\n",
        "\n",
        "Method for solving the RL cab be following:\n",
        "- Model based: Planning\n",
        "- Model free : Trial and Error"
      ],
      "metadata": {
        "id": "z_DamUcZAyCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Summary***\n",
        "There are 4 main component of RL system\n",
        "- Policy: Maps Actions from a state\n",
        "- Rewrad: Goal of RL agent, Short term\n",
        "- Value : Total anount of long term reward\n",
        "- Model: Predicts next State and Action"
      ],
      "metadata": {
        "id": "tHUxgODcBr3T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvoY4Cx5Eybw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}